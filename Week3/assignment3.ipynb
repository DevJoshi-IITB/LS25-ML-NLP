{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n# import evaluate\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:15:25.970042Z","iopub.execute_input":"2025-07-01T11:15:25.970740Z","iopub.status.idle":"2025-07-01T11:15:25.975206Z","shell.execute_reply.started":"2025-07-01T11:15:25.970713Z","shell.execute_reply":"2025-07-01T11:15:25.974206Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:15:28.704706Z","iopub.execute_input":"2025-07-01T11:15:28.705430Z","iopub.status.idle":"2025-07-01T11:15:31.508490Z","shell.execute_reply.started":"2025-07-01T11:15:28.705401Z","shell.execute_reply":"2025-07-01T11:15:31.507905Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:16:05.163435Z","iopub.execute_input":"2025-07-01T11:16:05.163757Z","iopub.status.idle":"2025-07-01T11:16:05.910155Z","shell.execute_reply.started":"2025-07-01T11:16:05.163734Z","shell.execute_reply":"2025-07-01T11:16:05.909309Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].shuffle(seed=69).select(range(1000))  \ntest_dataset = tokenized_datasets[\"test\"].shuffle(seed=69).select(range(200))\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:16:08.742954Z","iopub.execute_input":"2025-07-01T11:16:08.743627Z","iopub.status.idle":"2025-07-01T11:16:09.012621Z","shell.execute_reply.started":"2025-07-01T11:16:08.743602Z","shell.execute_reply":"2025-07-01T11:16:09.011629Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:16:11.942756Z","iopub.execute_input":"2025-07-01T11:16:11.943612Z","iopub.status.idle":"2025-07-01T11:16:11.948423Z","shell.execute_reply.started":"2025-07-01T11:16:11.943579Z","shell.execute_reply":"2025-07-01T11:16:11.947414Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=10,  # log every 10 steps\n    report_to=\"none\",  # disable wandb etc.\n    disable_tqdm=False,  # show progress bar\n    fp16=True,  # enable faster training if on GPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:16:36.022541Z","iopub.execute_input":"2025-07-01T11:16:36.023271Z","iopub.status.idle":"2025-07-01T11:16:36.058692Z","shell.execute_reply.started":"2025-07-01T11:16:36.023247Z","shell.execute_reply":"2025-07-01T11:16:36.058088Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:16:38.572264Z","iopub.execute_input":"2025-07-01T11:16:38.572542Z","iopub.status.idle":"2025-07-01T11:18:41.542884Z","shell.execute_reply.started":"2025-07-01T11:16:38.572525Z","shell.execute_reply":"2025-07-01T11:18:41.541813Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 01:57, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.424800</td>\n      <td>0.418680</td>\n      <td>0.810000</td>\n      <td>0.791209</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.223900</td>\n      <td>0.324792</td>\n      <td>0.870000</td>\n      <td>0.878505</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=64, training_loss=0.4148192321881652, metrics={'train_runtime': 122.1622, 'train_samples_per_second': 16.372, 'train_steps_per_second': 0.524, 'total_flos': 526222110720000.0, 'train_loss': 0.4148192321881652, 'epoch': 2.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(\"Evaluation Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:51:27.636286Z","iopub.execute_input":"2025-07-01T11:51:27.637245Z","iopub.status.idle":"2025-07-01T11:51:31.270111Z","shell.execute_reply.started":"2025-07-01T11:51:27.637215Z","shell.execute_reply":"2025-07-01T11:51:31.269500Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7/7 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.3247924745082855, 'eval_accuracy': 0.87, 'eval_f1': 0.8785046728971961, 'eval_runtime': 3.6229, 'eval_samples_per_second': 55.204, 'eval_steps_per_second': 1.932, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"./sentiment-bert\")\ntokenizer.save_pretrained(\"./sentiment-bert\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:53:49.758111Z","iopub.execute_input":"2025-07-01T11:53:49.758585Z","iopub.status.idle":"2025-07-01T11:53:51.109737Z","shell.execute_reply.started":"2025-07-01T11:53:49.758556Z","shell.execute_reply":"2025-07-01T11:53:51.108800Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./sentiment-bert/tokenizer_config.json',\n './sentiment-bert/special_tokens_map.json',\n './sentiment-bert/vocab.txt',\n './sentiment-bert/added_tokens.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def predict_sentiment(text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    model.to(device)\n\n    # Tokenize and move to device\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        prediction = torch.argmax(outputs.logits, dim=1).item()\n\n    return \"Positive\" if prediction == 1 else \"Negative\"\n\nsample_text = \"The movie was abysmal!\"\nprint(f\"Sample Prediction: {predict_sentiment(sample_text)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:55:57.114548Z","iopub.execute_input":"2025-07-01T11:55:57.114831Z","iopub.status.idle":"2025-07-01T11:55:57.137534Z","shell.execute_reply.started":"2025-07-01T11:55:57.114811Z","shell.execute_reply":"2025-07-01T11:55:57.136554Z"}},"outputs":[{"name":"stdout","text":"Sample Prediction: Negative\n","output_type":"stream"}],"execution_count":19}]}